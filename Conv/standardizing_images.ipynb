{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "standardizing_images.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP63UAk8b+uae54BIY09bwu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59aee9c1c509496ab5b1d566499f7d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eea8d36414aa405d852c4f0854bcb43d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_21be949116d64517bfe73ba267650974",
              "IPY_MODEL_9a733ff4ea2b4620ad3f1abef91a5af2"
            ]
          }
        },
        "eea8d36414aa405d852c4f0854bcb43d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "21be949116d64517bfe73ba267650974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d6135110981443daaa8674f206254c12",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e229b48c8a64676b1007bb8394237db"
          }
        },
        "9a733ff4ea2b4620ad3f1abef91a5af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3caf6be67ae448b8a92c0edab4dbf6aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 66765866.35it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fac88709c6b643ccbcc9483dc568889c"
          }
        },
        "d6135110981443daaa8674f206254c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e229b48c8a64676b1007bb8394237db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3caf6be67ae448b8a92c0edab4dbf6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fac88709c6b643ccbcc9483dc568889c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/stats453-deep_learning_torch/blob/main/Conv/standardizing_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jifM2UP9vPq"
      },
      "source": [
        "### **Standardizing Images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlqlT9Ix95gE"
      },
      "source": [
        "In this notebook we are going to calculate the mean and standard deviation of the training set and use it to standardize the training set. Transforms the images so that they have zero mean and unit variance accross channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxHz8xh1-avm"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VljBoC0v-njg"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4GuABkB--sO"
      },
      "source": [
        "### Settings and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfUyzFAz_D_b"
      },
      "source": [
        "## Settings \n",
        "\n",
        "# device \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "## hyper-parameters\n",
        "random_seed = 1\n",
        "learning_rate = 0.05\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "## architecture\n",
        "num_classes = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd0Z6i64_dpY"
      },
      "source": [
        "### Compute the mean and standard deviation for normalization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "59aee9c1c509496ab5b1d566499f7d43",
            "eea8d36414aa405d852c4f0854bcb43d",
            "21be949116d64517bfe73ba267650974",
            "9a733ff4ea2b4620ad3f1abef91a5af2",
            "d6135110981443daaa8674f206254c12",
            "0e229b48c8a64676b1007bb8394237db",
            "3caf6be67ae448b8a92c0edab4dbf6aa",
            "fac88709c6b643ccbcc9483dc568889c"
          ]
        },
        "id": "Ej3kxOwF_jYQ",
        "outputId": "51362fe0-5e2a-49c6-d63e-9a6e8d0ab385"
      },
      "source": [
        "### preliminary dataloader \n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False)\n",
        "\n",
        "train_mean = []\n",
        "train_std = []\n",
        "\n",
        "for i, image in enumerate(train_loader):\n",
        "    numpy_images = image[0].numpy()\n",
        "\n",
        "    batch_mean = np.mean(numpy_images, axis=(0, 2, 3))\n",
        "    batch_std = np.std(numpy_images, axis=(0, 2, 3))\n",
        "\n",
        "    train_mean.append(batch_mean)\n",
        "    train_std.append(batch_std)\n",
        "\n",
        "train_mean = torch.tensor(np.mean(train_mean, axis=0))\n",
        "train_std = torch.tensor(np.mean(train_std, axis=0))\n",
        "\n",
        "print('Mean: ', train_mean)\n",
        "print('Std: ', train_std)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59aee9c1c509496ab5b1d566499f7d43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Mean:  tensor([0.4914, 0.4822, 0.4465])\n",
            "Std:  tensor([0.2467, 0.2432, 0.2612])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YegklmnDCnvz"
      },
      "source": [
        "torch.ToTensor() method converts the images so that the values are in the range [0, 1] which is why we see the values of mean and std below 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swt8dG-pD_CE"
      },
      "source": [
        "### Standardizing Dataset Loader \n",
        "Now we can use the custom function to standardize the dataset according to the computed mean and standard deviation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3iLn56MEMnG"
      },
      "source": [
        "custom_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                       transforms.Normalize(train_mean, train_std)])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kf0LnHOEkmS",
        "outputId": "a00ec446-d017-45f6-d43b-fb223dd789bd"
      },
      "source": [
        "## preparing the dataset \n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='data',\n",
        "                               train=True,\n",
        "                               transform=custom_transform,\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='data',\n",
        "                               train=False,\n",
        "                               transform=custom_transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev0xqWnYFa2p",
        "outputId": "fc1bcf32-7ec3-4cef-83fd-7be60b5fc7c4"
      },
      "source": [
        "## checking the dataset \n",
        "\n",
        "for images, labels in train_loader:\n",
        "    print(f\"Feature batch dimensions: \", images.size())\n",
        "    print(f\"Target batch dimensions: \", labels.size())\n",
        "    break\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    print(f\"Feature batch dimensions: \", images.size())\n",
        "    print(f\"Target batch dimensions: \", labels.size())\n",
        "    break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature batch dimensions:  torch.Size([128, 3, 32, 32])\n",
            "Target batch dimensions:  torch.Size([128])\n",
            "Feature batch dimensions:  torch.Size([128, 3, 32, 32])\n",
            "Target batch dimensions:  torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mutoc6RaF2Dd"
      },
      "source": [
        "For the above batch check to see that the mean is roughly 0 and std is 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6sQ4l2SGTD3",
        "outputId": "177da6f0-2faa-490d-9080-497baeb4d3c6"
      },
      "source": [
        "print('Channel mean for the batch: ', torch.mean(images, dim=(0, 2, 3)))\n",
        "print('Channel std for the batch: ', torch.std(images, dim=(0, 2, 3)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Channel mean for the batch:  tensor([-0.0189, -0.0161, -0.0251])\n",
            "Channel std for the batch:  tensor([1.0145, 1.0248, 1.0157])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad2zzTG7G9ql"
      },
      "source": [
        "## model \n",
        "\n",
        "class ConvNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        ## calculating same padding: (w - k + 2*p)/s + 1 = o\n",
        "        ## => p = s (o - 1) - w + k / 2\n",
        "\n",
        "        # 32x32x3 => 32x32x4\n",
        "        self.conv_1 = torch.nn.Conv2d(in_channels=3,\n",
        "                                      out_channels=4,\n",
        "                                      kernel_size=(3, 3),\n",
        "                                      stride=(1, 1),\n",
        "                                      padding=(2)) # 1(32 - 1) - 32 + 3 / 2 = 2\n",
        "        # 32x32x4 => 16x16x4\n",
        "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                         stride=(2, 2),\n",
        "                                         padding=0) # 2(16-1) - 32 + 2 / 2 = 0\n",
        "        # 16x16x4 => 16x16x8\n",
        "        self.conv_2 = torch.nn.Conv2d(in_channels=4,\n",
        "                                      out_channels=8,\n",
        "                                      kernel_size=(3, 3),\n",
        "                                      stride=(1, 1),\n",
        "                                      padding=1) # 1(16 - 1) - 16 + 3 / 2 = 1\n",
        "        # 16x16x8 => 8x8x8\n",
        "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
        "                                         stride=(2, 2),\n",
        "                                         padding=0) # 2(8 - 1) - 16 + 2 / 2 = 0\n",
        "        # 8x8x8 => 10x10\n",
        "        self.linear_1 = torch.nn.Linear(in_features=8*8*8,\n",
        "                                        out_features=num_classes,\n",
        "                                        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool_1(out)\n",
        "\n",
        "        out = self.conv_2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool_2(out)\n",
        "\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        logits = self.linear_1(out)\n",
        "        probas = F.softmax(logits, dim=1)\n",
        "        return logits, probas \n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "model = ConvNet(num_classes)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27J6dq6pQITX"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lH25zaXQMwl",
        "outputId": "5235550c-ce55-4e16-efdd-efe24951403b"
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    accuracy, num_examples = 0, 0\n",
        "\n",
        "    for features, labels in data_loader:\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, probas = model(features)\n",
        "        _, predicted_labels = torch.max(probas, dim=1)\n",
        "        accuracy += (predicted_labels == labels).sum()\n",
        "        num_examples += features.size(0)\n",
        "\n",
        "    return accuracy.float() / num_examples * 100\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        ## forward and backward\n",
        "        logit, probas = model(features)\n",
        "\n",
        "        cost = F.cross_entropy(logit, labels)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        cost.backward()\n",
        "\n",
        "        ## update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        ## logging \n",
        "\n",
        "        if not batch_idx % 50:\n",
        "            print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}  |  Batch no: {batch_idx:03d}/{len(train_loader):03d}  \"\n",
        "                  f\"|  Cost: {cost:.4f}\")\n",
        "    \n",
        "    model.eval()\n",
        "    print(f\"Training Accuracy: {compute_accuracy(model, train_loader, device=device):.2f}%\", end=\"\")\n",
        "    print(f\"Time elapsed: {(time.time() - start_time)/60:.2f} min\")\n",
        "\n",
        "print(f\"Total time elapsed: {(time.time() - start_time) / 60:.2f} min\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/010  |  Batch no: 000/391  |  Cost: 2.3050\n",
            "Epoch: 001/010  |  Batch no: 050/391  |  Cost: 2.0531\n",
            "Epoch: 001/010  |  Batch no: 100/391  |  Cost: 2.0734\n",
            "Epoch: 001/010  |  Batch no: 150/391  |  Cost: 1.8774\n",
            "Epoch: 001/010  |  Batch no: 200/391  |  Cost: 1.8091\n",
            "Epoch: 001/010  |  Batch no: 250/391  |  Cost: 1.7566\n",
            "Epoch: 001/010  |  Batch no: 300/391  |  Cost: 1.5912\n",
            "Epoch: 001/010  |  Batch no: 350/391  |  Cost: 1.8087\n",
            "Training Accuracy: 42.41%Time elapsed: 0.34 min\n",
            "Epoch: 002/010  |  Batch no: 000/391  |  Cost: 1.5954\n",
            "Epoch: 002/010  |  Batch no: 050/391  |  Cost: 1.5798\n",
            "Epoch: 002/010  |  Batch no: 100/391  |  Cost: 1.5831\n",
            "Epoch: 002/010  |  Batch no: 150/391  |  Cost: 1.5584\n",
            "Epoch: 002/010  |  Batch no: 200/391  |  Cost: 1.5754\n",
            "Epoch: 002/010  |  Batch no: 250/391  |  Cost: 1.4309\n",
            "Epoch: 002/010  |  Batch no: 300/391  |  Cost: 1.3717\n",
            "Epoch: 002/010  |  Batch no: 350/391  |  Cost: 1.5754\n",
            "Training Accuracy: 49.53%Time elapsed: 0.69 min\n",
            "Epoch: 003/010  |  Batch no: 000/391  |  Cost: 1.4284\n",
            "Epoch: 003/010  |  Batch no: 050/391  |  Cost: 1.5276\n",
            "Epoch: 003/010  |  Batch no: 100/391  |  Cost: 1.5048\n",
            "Epoch: 003/010  |  Batch no: 150/391  |  Cost: 1.4973\n",
            "Epoch: 003/010  |  Batch no: 200/391  |  Cost: 1.4553\n",
            "Epoch: 003/010  |  Batch no: 250/391  |  Cost: 1.2761\n",
            "Epoch: 003/010  |  Batch no: 300/391  |  Cost: 1.3114\n",
            "Epoch: 003/010  |  Batch no: 350/391  |  Cost: 1.4560\n",
            "Training Accuracy: 52.66%Time elapsed: 1.03 min\n",
            "Epoch: 004/010  |  Batch no: 000/391  |  Cost: 1.2823\n",
            "Epoch: 004/010  |  Batch no: 050/391  |  Cost: 1.3465\n",
            "Epoch: 004/010  |  Batch no: 100/391  |  Cost: 1.5346\n",
            "Epoch: 004/010  |  Batch no: 150/391  |  Cost: 1.2012\n",
            "Epoch: 004/010  |  Batch no: 200/391  |  Cost: 1.3323\n",
            "Epoch: 004/010  |  Batch no: 250/391  |  Cost: 1.3134\n",
            "Epoch: 004/010  |  Batch no: 300/391  |  Cost: 1.3379\n",
            "Epoch: 004/010  |  Batch no: 350/391  |  Cost: 1.2422\n",
            "Training Accuracy: 53.15%Time elapsed: 1.38 min\n",
            "Epoch: 005/010  |  Batch no: 000/391  |  Cost: 1.5421\n",
            "Epoch: 005/010  |  Batch no: 050/391  |  Cost: 1.1276\n",
            "Epoch: 005/010  |  Batch no: 100/391  |  Cost: 1.2727\n",
            "Epoch: 005/010  |  Batch no: 150/391  |  Cost: 1.1371\n",
            "Epoch: 005/010  |  Batch no: 200/391  |  Cost: 1.2028\n",
            "Epoch: 005/010  |  Batch no: 250/391  |  Cost: 1.2779\n",
            "Epoch: 005/010  |  Batch no: 300/391  |  Cost: 1.3918\n",
            "Epoch: 005/010  |  Batch no: 350/391  |  Cost: 1.1304\n",
            "Training Accuracy: 54.94%Time elapsed: 1.72 min\n",
            "Epoch: 006/010  |  Batch no: 000/391  |  Cost: 1.2299\n",
            "Epoch: 006/010  |  Batch no: 050/391  |  Cost: 1.3044\n",
            "Epoch: 006/010  |  Batch no: 100/391  |  Cost: 1.0796\n",
            "Epoch: 006/010  |  Batch no: 150/391  |  Cost: 1.3661\n",
            "Epoch: 006/010  |  Batch no: 200/391  |  Cost: 1.2895\n",
            "Epoch: 006/010  |  Batch no: 250/391  |  Cost: 1.0819\n",
            "Epoch: 006/010  |  Batch no: 300/391  |  Cost: 1.2751\n",
            "Epoch: 006/010  |  Batch no: 350/391  |  Cost: 1.1648\n",
            "Training Accuracy: 54.79%Time elapsed: 2.06 min\n",
            "Epoch: 007/010  |  Batch no: 000/391  |  Cost: 1.2474\n",
            "Epoch: 007/010  |  Batch no: 050/391  |  Cost: 1.4333\n",
            "Epoch: 007/010  |  Batch no: 100/391  |  Cost: 1.1470\n",
            "Epoch: 007/010  |  Batch no: 150/391  |  Cost: 1.2028\n",
            "Epoch: 007/010  |  Batch no: 200/391  |  Cost: 1.2409\n",
            "Epoch: 007/010  |  Batch no: 250/391  |  Cost: 1.2644\n",
            "Epoch: 007/010  |  Batch no: 300/391  |  Cost: 1.4679\n",
            "Epoch: 007/010  |  Batch no: 350/391  |  Cost: 1.3038\n",
            "Training Accuracy: 58.26%Time elapsed: 2.41 min\n",
            "Epoch: 008/010  |  Batch no: 000/391  |  Cost: 1.2306\n",
            "Epoch: 008/010  |  Batch no: 050/391  |  Cost: 1.1504\n",
            "Epoch: 008/010  |  Batch no: 100/391  |  Cost: 1.2538\n",
            "Epoch: 008/010  |  Batch no: 150/391  |  Cost: 1.3697\n",
            "Epoch: 008/010  |  Batch no: 200/391  |  Cost: 1.0941\n",
            "Epoch: 008/010  |  Batch no: 250/391  |  Cost: 1.2670\n",
            "Epoch: 008/010  |  Batch no: 300/391  |  Cost: 1.1544\n",
            "Epoch: 008/010  |  Batch no: 350/391  |  Cost: 1.1326\n",
            "Training Accuracy: 59.35%Time elapsed: 2.76 min\n",
            "Epoch: 009/010  |  Batch no: 000/391  |  Cost: 1.1189\n",
            "Epoch: 009/010  |  Batch no: 050/391  |  Cost: 1.1132\n",
            "Epoch: 009/010  |  Batch no: 100/391  |  Cost: 1.2459\n",
            "Epoch: 009/010  |  Batch no: 150/391  |  Cost: 1.3555\n",
            "Epoch: 009/010  |  Batch no: 200/391  |  Cost: 1.1075\n",
            "Epoch: 009/010  |  Batch no: 250/391  |  Cost: 1.3475\n",
            "Epoch: 009/010  |  Batch no: 300/391  |  Cost: 1.0941\n",
            "Epoch: 009/010  |  Batch no: 350/391  |  Cost: 1.1134\n",
            "Training Accuracy: 57.32%Time elapsed: 3.10 min\n",
            "Epoch: 010/010  |  Batch no: 000/391  |  Cost: 1.2457\n",
            "Epoch: 010/010  |  Batch no: 050/391  |  Cost: 1.0854\n",
            "Epoch: 010/010  |  Batch no: 100/391  |  Cost: 1.2603\n",
            "Epoch: 010/010  |  Batch no: 150/391  |  Cost: 1.0458\n",
            "Epoch: 010/010  |  Batch no: 200/391  |  Cost: 1.1704\n",
            "Epoch: 010/010  |  Batch no: 250/391  |  Cost: 1.0706\n",
            "Epoch: 010/010  |  Batch no: 300/391  |  Cost: 1.1264\n",
            "Epoch: 010/010  |  Batch no: 350/391  |  Cost: 1.1757\n",
            "Training Accuracy: 57.32%Time elapsed: 3.44 min\n",
            "Total time elapsed: 3.44 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtmTCufEaOdd"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyRQrvvGfg6C",
        "outputId": "64c0b085-4577-4536-f344-c78b12eb2b7f"
      },
      "source": [
        "print(\"Test accuracy: %.2f%%\" % compute_accuracy(model, test_loader, device))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 55.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUumdR5CfwMY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}