{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_in_Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmXtJcbSrMoHVvq6I+PHoL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/stats453-deep_learning_torch/blob/main/Conv/Network_in_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybdNmtFSeVgQ"
      },
      "source": [
        "### Network in Network CIFAR-10 Classifier\n",
        "based on \n",
        "- Lin, Min, Qiang Chen, and Shuicheng Yan. \"Network in network.\" arXiv preprint arXiv:1312.4400 (2013)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfjMqQefeowD"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACnt3I75erVo"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G_-A3wGe3xL"
      },
      "source": [
        "## Model Settings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20sJurm5e8gy"
      },
      "source": [
        "### SETTINGS\n",
        "##########################\n",
        "\n",
        "# Hyperparameters\n",
        "RANDOM_SEED = 1\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 256\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Architecture\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Other\n",
        "DEVICE = \"cuda:0\"\n",
        "GRAYSCALE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU4QxjRYfALj",
        "outputId": "116ebf5c-d16a-4f4e-eafd-eef20d0e09c5"
      },
      "source": [
        "### CIFAR-10 Dataset\n",
        "\n",
        "train_indices = torch.arange(49000)\n",
        "valid_indices = torch.arange(49000, 50000)\n",
        "\n",
        "train_and_valid = datasets.CIFAR10(root='data',\n",
        "                                   train=True,\n",
        "                                   transform=transforms.ToTensor(),\n",
        "                                   download=True)\n",
        "\n",
        "train_dataset = Subset(train_and_valid, train_indices)\n",
        "valid_dataset = Subset(train_and_valid, valid_indices)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='data',\n",
        "                                train='False',\n",
        "                                transform=transforms.ToTensor())\n",
        "\n",
        "## DataLoader \n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True)\n",
        "\n",
        "valid_loader = DataLoader(valid_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=False,\n",
        "                          num_workers=4)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False,\n",
        "                         num_workers=4)\n",
        "\n",
        "### Checking dataset \n",
        "\n",
        "for image, labels in train_loader:\n",
        "    print(\"Image batch dimensions: \", image.size())\n",
        "    print(\"Labels batch dimensions: \", labels.size())\n",
        "    break\n",
        "\n",
        "for image, labels in valid_loader:\n",
        "    print(\"Image batch dimensions: \", image.size())\n",
        "    print(\"Labels batch dimensions: \", labels.size())\n",
        "    break\n",
        "\n",
        "for image, labels in test_loader:\n",
        "    print(\"Image batch dimensions: \", image.size())\n",
        "    print(\"Labels batch dimensions: \", labels.size())\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Image batch dimensions:  torch.Size([256, 3, 32, 32])\n",
            "Labels batch dimensions:  torch.Size([256])\n",
            "Image batch dimensions:  torch.Size([256, 3, 32, 32])\n",
            "Labels batch dimensions:  torch.Size([256])\n",
            "Image batch dimensions:  torch.Size([256, 3, 32, 32])\n",
            "Labels batch dimensions:  torch.Size([256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg_K82gDg8r8"
      },
      "source": [
        "### Model \n",
        "\n",
        "class NiN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(NiN, self).__init__()\n",
        "        self.classes = num_classes\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(160, 96, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(192, 10, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
        "\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.classifier(x)\n",
        "        logits = x.view(x.size(0), x.size(1))\n",
        "        probas = torch.softmax(logits, dim=1)\n",
        "        return logits, probas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBMSdT2DktmY"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "model = NiN(NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqNVi-KclAl_",
        "outputId": "b863509f-d0e0-4b27-b0f8-7a1f2f66165f"
      },
      "source": [
        "def compute_accuracy(model, data_loader, device):\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    for i, (features, targets) in enumerate(data_loader):\n",
        "            \n",
        "        features = features.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        logits, probas = model(features)\n",
        "        _, predicted_labels = torch.max(probas, 1)\n",
        "        num_examples += targets.size(0)\n",
        "        correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100\n",
        "    \n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# use random seed for reproducibility (here batch shuffling)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "    \n",
        "        ### PREPARE MINIBATCH\n",
        "        features = features.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "            \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits, probas = model(features)\n",
        "        cost = F.cross_entropy(logits, targets)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 120:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
        "                   f' Cost: {cost:.4f}')\n",
        "\n",
        "    # no need to build the computation graph for backprop when computing accuracy\n",
        "    with torch.set_grad_enabled(False):\n",
        "        train_acc = compute_accuracy(model, train_loader, device=DEVICE)\n",
        "        valid_acc = compute_accuracy(model, valid_loader, device=DEVICE)\n",
        "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
        "              f' | Validation Acc.: {valid_acc:.2f}%')\n",
        "        \n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Time elapsed: {elapsed:.2f} min')\n",
        "  \n",
        "elapsed = (time.time() - start_time)/60\n",
        "print(f'Total Training Time: {elapsed:.2f} min')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/050 | Batch 000/192 | Cost: 2.3047\n",
            "Epoch: 001/050 | Batch 120/192 | Cost: 2.1502\n",
            "Epoch: 001/050 Train Acc.: 23.81% | Validation Acc.: 22.70%\n",
            "Time elapsed: 0.53 min\n",
            "Epoch: 002/050 | Batch 000/192 | Cost: 2.0659\n",
            "Epoch: 002/050 | Batch 120/192 | Cost: 2.0531\n",
            "Epoch: 002/050 Train Acc.: 26.16% | Validation Acc.: 23.90%\n",
            "Time elapsed: 1.08 min\n",
            "Epoch: 003/050 | Batch 000/192 | Cost: 2.0478\n",
            "Epoch: 003/050 | Batch 120/192 | Cost: 1.8817\n",
            "Epoch: 003/050 Train Acc.: 34.45% | Validation Acc.: 33.80%\n",
            "Time elapsed: 1.65 min\n",
            "Epoch: 004/050 | Batch 000/192 | Cost: 1.9123\n",
            "Epoch: 004/050 | Batch 120/192 | Cost: 1.8682\n",
            "Epoch: 004/050 Train Acc.: 37.00% | Validation Acc.: 36.40%\n",
            "Time elapsed: 2.24 min\n",
            "Epoch: 005/050 | Batch 000/192 | Cost: 1.7955\n",
            "Epoch: 005/050 | Batch 120/192 | Cost: 1.8183\n",
            "Epoch: 005/050 Train Acc.: 39.05% | Validation Acc.: 38.20%\n",
            "Time elapsed: 2.81 min\n",
            "Epoch: 006/050 | Batch 000/192 | Cost: 1.6957\n",
            "Epoch: 006/050 | Batch 120/192 | Cost: 1.7128\n",
            "Epoch: 006/050 Train Acc.: 40.07% | Validation Acc.: 37.20%\n",
            "Time elapsed: 3.39 min\n",
            "Epoch: 007/050 | Batch 000/192 | Cost: 1.6929\n",
            "Epoch: 007/050 | Batch 120/192 | Cost: 1.5538\n",
            "Epoch: 007/050 Train Acc.: 45.35% | Validation Acc.: 45.10%\n",
            "Time elapsed: 3.97 min\n",
            "Epoch: 008/050 | Batch 000/192 | Cost: 1.5962\n",
            "Epoch: 008/050 | Batch 120/192 | Cost: 1.5136\n",
            "Epoch: 008/050 Train Acc.: 49.30% | Validation Acc.: 46.50%\n",
            "Time elapsed: 4.55 min\n",
            "Epoch: 009/050 | Batch 000/192 | Cost: 1.4586\n",
            "Epoch: 009/050 | Batch 120/192 | Cost: 1.4640\n",
            "Epoch: 009/050 Train Acc.: 50.99% | Validation Acc.: 49.20%\n",
            "Time elapsed: 5.12 min\n",
            "Epoch: 010/050 | Batch 000/192 | Cost: 1.3566\n",
            "Epoch: 010/050 | Batch 120/192 | Cost: 1.2288\n",
            "Epoch: 010/050 Train Acc.: 51.80% | Validation Acc.: 50.70%\n",
            "Time elapsed: 5.70 min\n",
            "Epoch: 011/050 | Batch 000/192 | Cost: 1.4963\n",
            "Epoch: 011/050 | Batch 120/192 | Cost: 1.3395\n",
            "Epoch: 011/050 Train Acc.: 51.64% | Validation Acc.: 47.70%\n",
            "Time elapsed: 6.28 min\n",
            "Epoch: 012/050 | Batch 000/192 | Cost: 1.3677\n",
            "Epoch: 012/050 | Batch 120/192 | Cost: 1.3632\n",
            "Epoch: 012/050 Train Acc.: 55.70% | Validation Acc.: 52.90%\n",
            "Time elapsed: 6.86 min\n",
            "Epoch: 013/050 | Batch 000/192 | Cost: 1.2966\n",
            "Epoch: 013/050 | Batch 120/192 | Cost: 1.3810\n",
            "Epoch: 013/050 Train Acc.: 55.04% | Validation Acc.: 53.20%\n",
            "Time elapsed: 7.43 min\n",
            "Epoch: 014/050 | Batch 000/192 | Cost: 1.2531\n",
            "Epoch: 014/050 | Batch 120/192 | Cost: 1.3512\n",
            "Epoch: 014/050 Train Acc.: 56.43% | Validation Acc.: 53.20%\n",
            "Time elapsed: 8.01 min\n",
            "Epoch: 015/050 | Batch 000/192 | Cost: 1.1761\n",
            "Epoch: 015/050 | Batch 120/192 | Cost: 1.1810\n",
            "Epoch: 015/050 Train Acc.: 57.28% | Validation Acc.: 56.30%\n",
            "Time elapsed: 8.59 min\n",
            "Epoch: 016/050 | Batch 000/192 | Cost: 1.1926\n",
            "Epoch: 016/050 | Batch 120/192 | Cost: 1.2241\n",
            "Epoch: 016/050 Train Acc.: 59.12% | Validation Acc.: 54.50%\n",
            "Time elapsed: 9.17 min\n",
            "Epoch: 017/050 | Batch 000/192 | Cost: 1.1500\n",
            "Epoch: 017/050 | Batch 120/192 | Cost: 1.3289\n",
            "Epoch: 017/050 Train Acc.: 59.93% | Validation Acc.: 56.50%\n",
            "Time elapsed: 9.74 min\n",
            "Epoch: 018/050 | Batch 000/192 | Cost: 1.0233\n",
            "Epoch: 018/050 | Batch 120/192 | Cost: 1.2944\n",
            "Epoch: 018/050 Train Acc.: 59.97% | Validation Acc.: 58.40%\n",
            "Time elapsed: 10.32 min\n",
            "Epoch: 019/050 | Batch 000/192 | Cost: 1.1273\n",
            "Epoch: 019/050 | Batch 120/192 | Cost: 1.1041\n",
            "Epoch: 019/050 Train Acc.: 61.20% | Validation Acc.: 58.80%\n",
            "Time elapsed: 10.90 min\n",
            "Epoch: 020/050 | Batch 000/192 | Cost: 1.0880\n",
            "Epoch: 020/050 | Batch 120/192 | Cost: 1.2368\n",
            "Epoch: 020/050 Train Acc.: 61.94% | Validation Acc.: 59.50%\n",
            "Time elapsed: 11.47 min\n",
            "Epoch: 021/050 | Batch 000/192 | Cost: 1.0947\n",
            "Epoch: 021/050 | Batch 120/192 | Cost: 1.0448\n",
            "Epoch: 021/050 Train Acc.: 62.53% | Validation Acc.: 58.70%\n",
            "Time elapsed: 12.05 min\n",
            "Epoch: 022/050 | Batch 000/192 | Cost: 0.9900\n",
            "Epoch: 022/050 | Batch 120/192 | Cost: 1.0529\n",
            "Epoch: 022/050 Train Acc.: 62.09% | Validation Acc.: 56.90%\n",
            "Time elapsed: 12.63 min\n",
            "Epoch: 023/050 | Batch 000/192 | Cost: 1.0021\n",
            "Epoch: 023/050 | Batch 120/192 | Cost: 1.0865\n",
            "Epoch: 023/050 Train Acc.: 63.84% | Validation Acc.: 58.70%\n",
            "Time elapsed: 13.20 min\n",
            "Epoch: 024/050 | Batch 000/192 | Cost: 1.0362\n",
            "Epoch: 024/050 | Batch 120/192 | Cost: 1.1207\n",
            "Epoch: 024/050 Train Acc.: 63.12% | Validation Acc.: 59.00%\n",
            "Time elapsed: 13.78 min\n",
            "Epoch: 025/050 | Batch 000/192 | Cost: 1.1452\n",
            "Epoch: 025/050 | Batch 120/192 | Cost: 1.0584\n",
            "Epoch: 025/050 Train Acc.: 64.17% | Validation Acc.: 59.30%\n",
            "Time elapsed: 14.35 min\n",
            "Epoch: 026/050 | Batch 000/192 | Cost: 1.0160\n",
            "Epoch: 026/050 | Batch 120/192 | Cost: 0.9432\n",
            "Epoch: 026/050 Train Acc.: 63.34% | Validation Acc.: 60.10%\n",
            "Time elapsed: 14.93 min\n",
            "Epoch: 027/050 | Batch 000/192 | Cost: 0.9841\n",
            "Epoch: 027/050 | Batch 120/192 | Cost: 0.9565\n",
            "Epoch: 027/050 Train Acc.: 63.36% | Validation Acc.: 57.90%\n",
            "Time elapsed: 15.51 min\n",
            "Epoch: 028/050 | Batch 000/192 | Cost: 1.0825\n",
            "Epoch: 028/050 | Batch 120/192 | Cost: 0.9389\n",
            "Epoch: 028/050 Train Acc.: 65.78% | Validation Acc.: 60.20%\n",
            "Time elapsed: 16.08 min\n",
            "Epoch: 029/050 | Batch 000/192 | Cost: 1.0030\n",
            "Epoch: 029/050 | Batch 120/192 | Cost: 1.1317\n",
            "Epoch: 029/050 Train Acc.: 66.10% | Validation Acc.: 60.00%\n",
            "Time elapsed: 16.66 min\n",
            "Epoch: 030/050 | Batch 000/192 | Cost: 1.0005\n",
            "Epoch: 030/050 | Batch 120/192 | Cost: 0.9476\n",
            "Epoch: 030/050 Train Acc.: 66.39% | Validation Acc.: 60.10%\n",
            "Time elapsed: 17.23 min\n",
            "Epoch: 031/050 | Batch 000/192 | Cost: 0.8844\n",
            "Epoch: 031/050 | Batch 120/192 | Cost: 0.9975\n",
            "Epoch: 031/050 Train Acc.: 65.69% | Validation Acc.: 59.30%\n",
            "Time elapsed: 17.81 min\n",
            "Epoch: 032/050 | Batch 000/192 | Cost: 1.0653\n",
            "Epoch: 032/050 | Batch 120/192 | Cost: 1.1305\n",
            "Epoch: 032/050 Train Acc.: 66.32% | Validation Acc.: 59.40%\n",
            "Time elapsed: 18.38 min\n",
            "Epoch: 033/050 | Batch 000/192 | Cost: 1.0275\n",
            "Epoch: 033/050 | Batch 120/192 | Cost: 0.9531\n",
            "Epoch: 033/050 Train Acc.: 67.33% | Validation Acc.: 60.10%\n",
            "Time elapsed: 18.96 min\n",
            "Epoch: 034/050 | Batch 000/192 | Cost: 0.8814\n",
            "Epoch: 034/050 | Batch 120/192 | Cost: 1.0143\n",
            "Epoch: 034/050 Train Acc.: 65.47% | Validation Acc.: 58.90%\n",
            "Time elapsed: 19.53 min\n",
            "Epoch: 035/050 | Batch 000/192 | Cost: 1.0583\n",
            "Epoch: 035/050 | Batch 120/192 | Cost: 0.9753\n",
            "Epoch: 035/050 Train Acc.: 67.71% | Validation Acc.: 59.80%\n",
            "Time elapsed: 20.10 min\n",
            "Epoch: 036/050 | Batch 000/192 | Cost: 0.8451\n",
            "Epoch: 036/050 | Batch 120/192 | Cost: 0.9027\n",
            "Epoch: 036/050 Train Acc.: 67.57% | Validation Acc.: 59.20%\n",
            "Time elapsed: 20.68 min\n",
            "Epoch: 037/050 | Batch 000/192 | Cost: 0.9269\n",
            "Epoch: 037/050 | Batch 120/192 | Cost: 0.8384\n",
            "Epoch: 037/050 Train Acc.: 65.30% | Validation Acc.: 58.20%\n",
            "Time elapsed: 21.25 min\n",
            "Epoch: 038/050 | Batch 000/192 | Cost: 1.0226\n",
            "Epoch: 038/050 | Batch 120/192 | Cost: 0.7398\n",
            "Epoch: 038/050 Train Acc.: 67.46% | Validation Acc.: 59.60%\n",
            "Time elapsed: 21.83 min\n",
            "Epoch: 039/050 | Batch 000/192 | Cost: 0.8807\n",
            "Epoch: 039/050 | Batch 120/192 | Cost: 0.8119\n",
            "Epoch: 039/050 Train Acc.: 67.68% | Validation Acc.: 58.90%\n",
            "Time elapsed: 22.40 min\n",
            "Epoch: 040/050 | Batch 000/192 | Cost: 0.9084\n",
            "Epoch: 040/050 | Batch 120/192 | Cost: 0.7793\n",
            "Epoch: 040/050 Train Acc.: 69.98% | Validation Acc.: 62.70%\n",
            "Time elapsed: 22.98 min\n",
            "Epoch: 041/050 | Batch 000/192 | Cost: 0.8893\n",
            "Epoch: 041/050 | Batch 120/192 | Cost: 0.8867\n",
            "Epoch: 041/050 Train Acc.: 70.05% | Validation Acc.: 62.30%\n",
            "Time elapsed: 23.56 min\n",
            "Epoch: 042/050 | Batch 000/192 | Cost: 0.8259\n",
            "Epoch: 042/050 | Batch 120/192 | Cost: 0.9158\n",
            "Epoch: 042/050 Train Acc.: 68.32% | Validation Acc.: 59.10%\n",
            "Time elapsed: 24.13 min\n",
            "Epoch: 043/050 | Batch 000/192 | Cost: 0.8662\n",
            "Epoch: 043/050 | Batch 120/192 | Cost: 0.8896\n",
            "Epoch: 043/050 Train Acc.: 69.79% | Validation Acc.: 61.20%\n",
            "Time elapsed: 24.71 min\n",
            "Epoch: 044/050 | Batch 000/192 | Cost: 0.7466\n",
            "Epoch: 044/050 | Batch 120/192 | Cost: 0.8392\n",
            "Epoch: 044/050 Train Acc.: 70.69% | Validation Acc.: 61.60%\n",
            "Time elapsed: 25.28 min\n",
            "Epoch: 045/050 | Batch 000/192 | Cost: 0.7798\n",
            "Epoch: 045/050 | Batch 120/192 | Cost: 0.9295\n",
            "Epoch: 045/050 Train Acc.: 70.86% | Validation Acc.: 63.10%\n",
            "Time elapsed: 25.86 min\n",
            "Epoch: 046/050 | Batch 000/192 | Cost: 0.8885\n",
            "Epoch: 046/050 | Batch 120/192 | Cost: 0.7705\n",
            "Epoch: 046/050 Train Acc.: 71.06% | Validation Acc.: 62.40%\n",
            "Time elapsed: 26.43 min\n",
            "Epoch: 047/050 | Batch 000/192 | Cost: 0.7433\n",
            "Epoch: 047/050 | Batch 120/192 | Cost: 0.7724\n",
            "Epoch: 047/050 Train Acc.: 70.87% | Validation Acc.: 63.30%\n",
            "Time elapsed: 27.01 min\n",
            "Epoch: 048/050 | Batch 000/192 | Cost: 0.7262\n",
            "Epoch: 048/050 | Batch 120/192 | Cost: 0.9263\n",
            "Epoch: 048/050 Train Acc.: 71.33% | Validation Acc.: 60.90%\n",
            "Time elapsed: 27.58 min\n",
            "Epoch: 049/050 | Batch 000/192 | Cost: 0.7846\n",
            "Epoch: 049/050 | Batch 120/192 | Cost: 0.8128\n",
            "Epoch: 049/050 Train Acc.: 71.05% | Validation Acc.: 60.80%\n",
            "Time elapsed: 28.16 min\n",
            "Epoch: 050/050 | Batch 000/192 | Cost: 0.8088\n",
            "Epoch: 050/050 | Batch 120/192 | Cost: 0.9092\n",
            "Epoch: 050/050 Train Acc.: 71.03% | Validation Acc.: 61.90%\n",
            "Time elapsed: 28.74 min\n",
            "Total Training Time: 28.74 min\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOZoZ3L1lair",
        "outputId": "3b20f17a-4f35-44bf-d292-1fe1aa40fc96"
      },
      "source": [
        "print(f\"Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 70.93%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}