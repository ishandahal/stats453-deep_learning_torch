{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L2_regularization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPmxCFncanw6Kzk85jKqgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/stats453-deep_learning_torch/blob/main/Regularization/L2_regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_gIZGMAsAhB"
      },
      "source": [
        "### Logistic Regression with L2 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REFG1ggdsfFX"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-6gEMZpsg5l"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLEOsuQ_soHa"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "LAMBDA = 2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNhgwDrHtHJN"
      },
      "source": [
        "Preping dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG9FO-DDtKms"
      },
      "source": [
        "data = np.genfromtxt('https://raw.githubusercontent.com/rasbt/stat453-deep-learning-ss20/master/L07-logistic/code/data/toydata.txt',\n",
        "                     delimiter='\\t')\n",
        "x = data[:, :2].astype(np.float32)\n",
        "y = data[:, 2].astype(np.int64)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlNN1Y7jtlvF"
      },
      "source": [
        "np.random.seed(123)\n",
        "idx = np.arange(x.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "X_test, y_test = x[idx[:25]], y[idx[:25]]\n",
        "X_train, y_train = x[idx[25:]], y[idx[25:]]\n",
        "mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)\n",
        "X_train, X_test = (X_train - mu) / sigma, (X_test - mu) / sigma "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "274zLroWurk_",
        "outputId": "82b11970-40d6-4e3b-ad9e-78ef7f45f734"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(7.5, 3))\n",
        "ax[0].scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1])\n",
        "ax[0].scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1])\n",
        "ax[1].scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1])\n",
        "ax[1].scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1])\n",
        "plt.xlim([x[:, 0].min()-0.5, x[:, 0].max()+0.5])\n",
        "plt.ylim([x[:, 1].min()-0.5, x[:, 1].max()+0.5])\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 540x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wHQKwKZwIt8"
      },
      "source": [
        "## L2 Regularized Logistic Regression via ```weight_decay```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWLD_SE-xfLi",
        "outputId": "3ee06af4-b4fb-4b7b-8287-8e40c6747d26"
      },
      "source": [
        "def custom_where(cond, x_1, x_2):\n",
        "    return (cond * x_1) + ((1-cond) * x_2)\n",
        "\n",
        "class LogisticRegression(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(num_features, 1)\n",
        "\n",
        "        self.linear.weight.detach().zero_()\n",
        "        self.linear.bias.detach().zero_()\n",
        "        ## We don't need random small weights since it is logistic regression \n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        probas = torch.sigmoid(logits)\n",
        "        return probas \n",
        "\n",
        "model = LogisticRegression(num_features=2)\n",
        "\n",
        "## Applying L2 regularization \n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr=0.1,\n",
        "                            weight_decay=LAMBDA)\n",
        "\n",
        "def comp_accuracy(label_var, pred_probas):\n",
        "    pred_labels = custom_where((pred_probas > 0.5).float(), 1, 0).view(-1)\n",
        "    acc = torch.sum(pred_labels == label_var.view(-1)).float() / label_var.size(0)\n",
        "    return acc\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float, device=device).view(-1, 1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #compute outputs \n",
        "    out = model(X_train_tensor)\n",
        "\n",
        "    # Compute gradients \n",
        "    cost = F.binary_cross_entropy(out, y_train_tensor, reduction='sum')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "\n",
        "    ## updating weights\n",
        "    optimizer.step()\n",
        "\n",
        "    ## Loggin \n",
        "    pred_probas = model(X_train_tensor)\n",
        "    acc = comp_accuracy(y_train_tensor, pred_probas)\n",
        "    print(f\"Epoch: {epoch+1:03d}\", end=\"\")\n",
        "    print(f\"  |  Train Acc: {acc:.3f}\", end=\"\")\n",
        "    print(f\"  |  Cost: {F.binary_cross_entropy(pred_probas, y_train_tensor):.3f}\")\n",
        "\n",
        "print(\"\\nModel Parameters:\")\n",
        "print(f\"    Weights: {model.linear.weight}\")\n",
        "print(f\"    Bias: {model.linear.bias}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001  |  Train Acc: 0.973  |  Cost: 0.055\n",
            "Epoch: 002  |  Train Acc: 0.973  |  Cost: 0.065\n",
            "Epoch: 003  |  Train Acc: 0.973  |  Cost: 0.080\n",
            "Epoch: 004  |  Train Acc: 0.973  |  Cost: 0.094\n",
            "Epoch: 005  |  Train Acc: 0.973  |  Cost: 0.104\n",
            "Epoch: 006  |  Train Acc: 0.973  |  Cost: 0.108\n",
            "Epoch: 007  |  Train Acc: 0.973  |  Cost: 0.110\n",
            "Epoch: 008  |  Train Acc: 0.973  |  Cost: 0.111\n",
            "Epoch: 009  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 010  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 011  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 012  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 013  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 014  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 015  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 016  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 017  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 018  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 019  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 020  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 021  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 022  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 023  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 024  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 025  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 026  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 027  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 028  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 029  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 030  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "\n",
            "Model Parameters:\n",
            "    Weights: Parameter containing:\n",
            "tensor([[1.7546, 1.5997]], requires_grad=True)\n",
            "    Bias: Parameter containing:\n",
            "tensor([-0.0401], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHaovva_2BFp",
        "outputId": "7676327f-d98c-4e61-c503-d706a98830f4"
      },
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float, device=device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float, device=device)\n",
        "\n",
        "pred_probas = model(X_test_tensor)\n",
        "test_acc = comp_accuracy(y_test_tensor, pred_probas)\n",
        "\n",
        "print(f\"\\nTest set accuracy: {test_acc*100:.2f}%\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy: 96.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9hv2t9O3KJA"
      },
      "source": [
        "## L2 Regularization Logistic Regression via Manual Regularization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usTQjo2D3i-t",
        "outputId": "a1e19575-8189-4556-fe28-4114dfd8976b"
      },
      "source": [
        "model = LogisticRegression(num_features=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    ## compute outputs \n",
        "    out = model(X_train_tensor)\n",
        "\n",
        "    ## comute gradients \n",
        "    ## applying L2 regularization (weight decay)\n",
        "    cost = F.binary_cross_entropy(out, y_train_tensor, reduction='sum')\n",
        "    cost = cost + 0.5 * LAMBDA * torch.mm(model.linear.weight,\n",
        "                                    model.linear.weight.t())\n",
        "    \n",
        "    # pytorch regularizes the bias as well so if we want to replicate auto SGD's \n",
        "    # output we need to regularize the bias as well \n",
        "    cost += 0.5 * LAMBDA * (model.linear.bias**2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "\n",
        "    # update the weights \n",
        "    optimizer.step()\n",
        "\n",
        "    #Logging \n",
        "    pred_probas = model(X_train_tensor)\n",
        "    acc = comp_accuracy(y_train_tensor, pred_probas)\n",
        "    print(f\"Epoch: {epoch+1:03d}\", end=\"\")\n",
        "    print(f\"  |  Train Acc: {acc:.3f}\", end=\"\")\n",
        "    print(f\"  |  Cost: {F.binary_cross_entropy(pred_probas, y_train_tensor):.3f}\")\n",
        "\n",
        "print(\"\\nModel Parameters:\")\n",
        "print(f\"    Weights: {model.linear.weight}\")\n",
        "print(f\"    Bias: {model.linear.bias}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001  |  Train Acc: 0.973  |  Cost: 0.055\n",
            "Epoch: 002  |  Train Acc: 0.973  |  Cost: 0.065\n",
            "Epoch: 003  |  Train Acc: 0.973  |  Cost: 0.080\n",
            "Epoch: 004  |  Train Acc: 0.973  |  Cost: 0.094\n",
            "Epoch: 005  |  Train Acc: 0.973  |  Cost: 0.104\n",
            "Epoch: 006  |  Train Acc: 0.973  |  Cost: 0.108\n",
            "Epoch: 007  |  Train Acc: 0.973  |  Cost: 0.110\n",
            "Epoch: 008  |  Train Acc: 0.973  |  Cost: 0.111\n",
            "Epoch: 009  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 010  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 011  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 012  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 013  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 014  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 015  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 016  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 017  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 018  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 019  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 020  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 021  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 022  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 023  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 024  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 025  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 026  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 027  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 028  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 029  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "Epoch: 030  |  Train Acc: 0.973  |  Cost: 0.112\n",
            "\n",
            "Model Parameters:\n",
            "    Weights: Parameter containing:\n",
            "tensor([[1.7546, 1.5997]], requires_grad=True)\n",
            "    Bias: Parameter containing:\n",
            "tensor([-0.0401], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HMcyPqq5QWz",
        "outputId": "6729e21d-f344-4fc8-bc6c-09c7d58084f3"
      },
      "source": [
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float, device=device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float, device=device)\n",
        "\n",
        "pred_probas = model(X_test_tensor)\n",
        "test_acc = comp_accuracy(y_test_tensor, pred_probas)\n",
        "\n",
        "print(f\"\\nTest set accuracy: {test_acc*100:.2f}%\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set accuracy: 96.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-Do84N-5SzY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}