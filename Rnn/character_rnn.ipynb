{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_rnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHYsrvrvXOBd2ZH7OQtdjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/stats453-deep_learning_torch/blob/main/Rnn/character_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxW708uvZWUG"
      },
      "source": [
        "### Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqB30P05ZboQ"
      },
      "source": [
        "Simple character RNN that generates bits of text based on a novel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaQXQVS0aFZ6",
        "outputId": "807737e6-98cf-489f-dc4f-d88a6ec822f6"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JJO01GSZlP2"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import time\n",
        "import random \n",
        "import unidecode\n",
        "import string\n",
        "import random \n",
        "import re\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW_nsleRZ8Bp"
      },
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "TEXT_PORTION_SIZE = 200\n",
        "\n",
        "NUM_ITER = 20000\n",
        "LEARNING_RATE = 0.005\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 100\n",
        "NUM_HIDDEN = 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rflHl2DBaLOu"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG5cBWImaa1O",
        "outputId": "47fbf448-6a8b-4df2-c90d-8694c91e493e"
      },
      "source": [
        "!wget http://www.gutenberg.org/files/98/98-0.txt"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-29 04:31:09--  http://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807615 (789K) [text/plain]\n",
            "Saving to: ‘98-0.txt.1’\n",
            "\n",
            "98-0.txt.1          100%[===================>] 788.69K  3.23MB/s    in 0.2s    \n",
            "\n",
            "2020-12-29 04:31:09 (3.23 MB/s) - ‘98-0.txt.1’ saved [807615/807615]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Es_Hfyaemn"
      },
      "source": [
        "Convert all characters to ASCii characters provided by ```string.printable```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6Anycg2EasFH",
        "outputId": "8f02883c-5e4a-4ebf-ddbb-f87d35364daa"
      },
      "source": [
        "string.printable"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG5AanFEavW-",
        "outputId": "ed6f7cb3-71c6-47c0-fff6-6e19cb0bdc16"
      },
      "source": [
        "with open('./98-0.txt', 'r') as f:\n",
        "    textfile = f.read()\n",
        "\n",
        "# # convert special characters \n",
        "textfile = unidecode.unidecode(textfile)\n",
        "\n",
        "# strip extra whitespaces \n",
        "textfile = re.sub(' +', ' ', textfile)\n",
        "\n",
        "TEXT_LENGTH = len(textfile)\n",
        "\n",
        "print(f\"Number of characters in text: {TEXT_LENGTH}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters in text: 776911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwbW8MZabThw"
      },
      "source": [
        "Divide the text into smaller portions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUThBLl7bm3h",
        "outputId": "2c98cdf4-75ec-40e2-b607-4cb0913df6f5"
      },
      "source": [
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "def random_portion(textfile):\n",
        "    start_index = random.randint(0, TEXT_LENGTH - TEXT_PORTION_SIZE)\n",
        "    end_index = start_index + TEXT_PORTION_SIZE + 1\n",
        "    return textfile[start_index:end_index]\n",
        "\n",
        "print(random_portion(textfile))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "and dancing, a dozen\n",
            "together. When the wine was gone, and the places where it had been\n",
            "most abundant were raked into a gridiron-pattern by fingers, these\n",
            "demonstrations ceased, as suddenly as they had\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzIVJGL5dINC"
      },
      "source": [
        "Define a function that converts characters into tensors of integers (type long)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL6V3NwfdY1J",
        "outputId": "38260a60-f015-4670-93f5-57d2fdefe391"
      },
      "source": [
        "def char_to_tensor(text):\n",
        "    lst = [string.printable.index(c) for c in text]\n",
        "    tensor = torch.tensor(lst).long()\n",
        "    return tensor\n",
        "\n",
        "print(char_to_tensor('abcDEF'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guwYSsy8dxec"
      },
      "source": [
        "Putting it together to make a function that draws random batches for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLCFvvQpd4Fp"
      },
      "source": [
        "def draw_random_sample(textfile):\n",
        "    text_long = char_to_tensor(random_portion(textfile))\n",
        "    inputs = text_long[:-1]\n",
        "    targets = text_long[1:]\n",
        "    return inputs, targets"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyoAknKHfHiA",
        "outputId": "907487e4-6aa1-4095-a44e-11cfa9b1ebe6"
      },
      "source": [
        "draw_random_sample(textfile)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([17, 14, 94, 11, 10, 23, 20, 73, 94, 10, 23, 13, 94, 27, 14, 31, 14, 10,\n",
              "         21, 94, 29, 24, 94, 48, 27, 75, 94, 47, 24, 27, 27, 34, 94, 29, 17, 14,\n",
              "         94, 11, 27, 18, 16, 17, 29, 23, 14, 28, 28, 96, 24, 15, 94, 29, 17, 14,\n",
              "         94, 54, 24, 17, 24, 94, 17, 24, 27, 18, 35, 24, 23, 75, 94, 54, 24, 73,\n",
              "         94, 17, 14, 94, 25, 30, 28, 17, 14, 13, 94, 24, 25, 14, 23, 94, 29, 17,\n",
              "         14, 94, 13, 24, 24, 27, 94, 32, 18, 29, 17, 94, 29, 17, 14, 94, 32, 14,\n",
              "         10, 20, 94, 27, 10, 29, 29, 21, 14, 96, 18, 23, 94, 18, 29, 28, 94, 29,\n",
              "         17, 27, 24, 10, 29, 73, 94, 28, 29, 30, 22, 11, 21, 14, 13, 94, 13, 24,\n",
              "         32, 23, 94, 29, 17, 14, 94, 29, 32, 24, 94, 28, 29, 14, 25, 28, 73, 94,\n",
              "         16, 24, 29, 94, 25, 10, 28, 29, 94, 29, 17, 14, 94, 29, 32, 24, 94, 10,\n",
              "         23, 12, 18, 14, 23, 29, 96, 12, 10, 28, 17, 18, 14, 27, 28, 73, 94, 10,\n",
              "         23, 13]),\n",
              " tensor([14, 94, 11, 10, 23, 20, 73, 94, 10, 23, 13, 94, 27, 14, 31, 14, 10, 21,\n",
              "         94, 29, 24, 94, 48, 27, 75, 94, 47, 24, 27, 27, 34, 94, 29, 17, 14, 94,\n",
              "         11, 27, 18, 16, 17, 29, 23, 14, 28, 28, 96, 24, 15, 94, 29, 17, 14, 94,\n",
              "         54, 24, 17, 24, 94, 17, 24, 27, 18, 35, 24, 23, 75, 94, 54, 24, 73, 94,\n",
              "         17, 14, 94, 25, 30, 28, 17, 14, 13, 94, 24, 25, 14, 23, 94, 29, 17, 14,\n",
              "         94, 13, 24, 24, 27, 94, 32, 18, 29, 17, 94, 29, 17, 14, 94, 32, 14, 10,\n",
              "         20, 94, 27, 10, 29, 29, 21, 14, 96, 18, 23, 94, 18, 29, 28, 94, 29, 17,\n",
              "         27, 24, 10, 29, 73, 94, 28, 29, 30, 22, 11, 21, 14, 13, 94, 13, 24, 32,\n",
              "         23, 94, 29, 17, 14, 94, 29, 32, 24, 94, 28, 29, 14, 25, 28, 73, 94, 16,\n",
              "         24, 29, 94, 25, 10, 28, 29, 94, 29, 17, 14, 94, 29, 32, 24, 94, 10, 23,\n",
              "         12, 18, 14, 23, 29, 96, 12, 10, 28, 17, 18, 14, 27, 28, 73, 94, 10, 23,\n",
              "         13, 94]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J1F00VyfKYw"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HtfGm_AfVXo"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, \n",
        "                 hid_dim, out_dim, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embed = nn.Embedding(input_dim, emb_dim)\n",
        "        self.gru = nn.GRU(input_size=emb_dim,\n",
        "                          hidden_size=hid_dim,\n",
        "                          num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hid_dim, out_dim)\n",
        "        self.init_hidden = nn.Parameter(torch.zeros(\n",
        "                                            num_layers, 1, hid_dim))\n",
        "        \n",
        "    def forward(self, features, hidden):\n",
        "        embedded = self.embed(features.view(1, -1))\n",
        "        output, hidden = self.gru(embedded.view(1, 1, -1), hidden)\n",
        "        output = self.fc(output.view(1, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_zero_state(self):\n",
        "        init_hidden = torch.zeros(self.num_layers, 1, self.hid_dim).to(DEVICE)\n",
        "        return init_hidden        "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuVa9-VChyRq"
      },
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(len(string.printable), EMBEDDING_DIM, HIDDEN_DIM,\n",
        "            len(string.printable), NUM_HIDDEN)\n",
        "model = model.to(DEVICE)\n",
        "optimizer =  torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrRI9zWXsVs6"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pq0qy21sk5n"
      },
      "source": [
        "def evaluate(model, prime_str='A', predict_len=100, temperature=0.8):\n",
        "\n",
        "    hidden = model.init_zero_state()\n",
        "    prime_input = char_to_tensor(prime_str)\n",
        "    predicted = prime_str\n",
        "\n",
        "    ## Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = model(prime_input[p].to(DEVICE), hidden.to(DEVICE))\n",
        "    inp = prime_input[-1]\n",
        "\n",
        "    for p in range(predict_len):\n",
        "        output, hidden = model(inp.to(DEVICE), hidden.to(DEVICE))\n",
        "\n",
        "        # Sample for the network as a multinomial distribution \n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = string.printable[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = char_to_tensor(predicted_char)\n",
        "    \n",
        "    return predicted"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqoD0V3MulaO",
        "outputId": "3041cfdf-b6b2-4974-fe4a-d1b14c7fbfec"
      },
      "source": [
        "start_time = time.time()\n",
        "for iteration in range(NUM_ITER):\n",
        "\n",
        "    ## Forward and Back Prop\n",
        "\n",
        "    hidden = model.init_zero_state()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = 0.\n",
        "    inputs, targets = draw_random_sample(textfile)\n",
        "    inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
        "    for c in range(TEXT_PORTION_SIZE):\n",
        "        outputs, hidden = model(inputs[c], hidden)\n",
        "        loss += F.cross_entropy(outputs, targets[c].view(1))\n",
        "\n",
        "    loss /= TEXT_PORTION_SIZE\n",
        "    loss.backward()\n",
        "\n",
        "    ## Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    ## logging \n",
        "    with torch.set_grad_enabled(False):\n",
        "        if not iteration % 1000:\n",
        "            print(f\"Time elapsed: {(time.time() - start_time)/60:.2f} min\")\n",
        "            print(f\"Iteration {iteration} | Loss {loss.item():.2f}\\n\\n\")\n",
        "            print(evaluate(model, 'Th', 200), '\\n')\n",
        "            print(50*'=')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed: 0.00 min\n",
            "Iteration 0 | Loss 4.60\n",
            "\n",
            "\n",
            "Th>\\T-\\`#Xx&fW6B^Lm1^uAqrU?w8=kH;,XATEO\n",
            "+pgDgbu\r\f=U}kY3ysMJ\\2<@Gh:`|ch1@\u000b t\rA^tj|A0\"Y\n",
            ")T,\fI?G%CyJQ^\r$G)Oqw(37-Yq9lFo2{6!*9tO5?E[#M4)!s@2)iRxvC8Wx:\"$6zf?G98c8\fHKP79/TWI\fYv\r,d1_L)G]RxymA^8^Fbr\"a^nD\fu_aGc( \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 3.04 min\n",
            "Iteration 1000 | Loss 1.94\n",
            "\n",
            "\n",
            "The that low was fatter out good reppentare stook bether) halys ortion, but Foun in not monser the was a carges on in but\n",
            "refured acponster of the one fow of her ot that the sconters to him bundry doon  \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 6.13 min\n",
            "Iteration 2000 | Loss 1.64\n",
            "\n",
            "\n",
            "There, from in sain\n",
            "why. The rose. Then courting of a must and steable of presibmor. I\n",
            "shook it in the distared the startely the contory, worn, who not of they to be pain the drew not\n",
            "that while others  \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 9.20 min\n",
            "Iteration 3000 | Loss 1.73\n",
            "\n",
            "\n",
            "Throw intial bus, and frongly and were his bow, its before bedowed\n",
            "him asked that a very the long at the frogisch wife of him, and the\n",
            "is me of armined; and and of fae preseed it wind the had from its p \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 12.24 min\n",
            "Iteration 4000 | Loss 1.90\n",
            "\n",
            "\n",
            "This a know which\n",
            "I whot with the deant hand or question of the could her\n",
            "morriced you will pick. His will will how and not suffer, and and of will\n",
            "of I had long at the more and that the duning\n",
            "chait so \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 15.28 min\n",
            "Iteration 5000 | Loss 1.73\n",
            "\n",
            "\n",
            "There wheir had three he dear\n",
            "ready at him.\n",
            "\n",
            "\"Not sir?\" said me which which had on again of his donal\n",
            "thim off of his featitebed, and sattly have on him, \"Mr.\n",
            "Some on him, I don't which comshie hailed f \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 18.31 min\n",
            "Iteration 6000 | Loss 1.73\n",
            "\n",
            "\n",
            "The Strawoner\n",
            "by prespression lumbed shoe found, tring to rushessed be amond the eace not her\n",
            "days of her shall as cried sirt, strivegration for neverse tho is by the since of the\n",
            "were times, with a kin \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 21.37 min\n",
            "Iteration 7000 | Loss 1.66\n",
            "\n",
            "\n",
            "Ther with his poor and such\n",
            "this collself to do not with a mind stragate good in this musting out to you\n",
            "spoken one the confing of this murpuring Joir spoiloined teemark so\n",
            "grath to oun friend to strang \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 24.42 min\n",
            "Iteration 8000 | Loss 1.66\n",
            "\n",
            "\n",
            "They hand, and\n",
            "many his rapson call compleaus in lived, and and pason of And a be on who are\n",
            "cophoud, eel which he have murchenwhlat a prople crowe of better lale ourtity of my crather\n",
            "to morsed to the  \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 27.48 min\n",
            "Iteration 9000 | Loss 1.62\n",
            "\n",
            "\n",
            "The Lucie's\n",
            "father sout to tome young Jacques. I am at the time, when the wretted\n",
            "to of oversal court, there have be see the requt be the laid\n",
            "cown not marked fillage, on the prisoned is a very, and pas \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 30.51 min\n",
            "Iteration 10000 | Loss 1.72\n",
            "\n",
            "\n",
            "Theress ocching.\n",
            "They of as her escrection: the other, Maday, and they who\n",
            "new was pulleticied, and out here they will deying with they meant of the\n",
            "matting and not a place and is the at they trumber, f \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 33.55 min\n",
            "Iteration 11000 | Loss 1.61\n",
            "\n",
            "\n",
            "Ther.\n",
            "\n",
            "Who have vounfable wihe priver of purvered, so from businemenfult on his\n",
            "it becaned of pairs, and him, there with their.\n",
            "\n",
            "\"Mon at the reprecasting of selizervice of corminationary with survick in \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 36.58 min\n",
            "Iteration 12000 | Loss 1.71\n",
            "\n",
            "\n",
            "They cipesp breating\n",
            "that day; again. Him that you fee, I was widence. I\n",
            " dougn the lowered him the so have, and Solove\n",
            "strong it of the rement he more deep the calls do you have the looks of\n",
            "pousectal  \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 39.62 min\n",
            "Iteration 13000 | Loss 1.63\n",
            "\n",
            "\n",
            "Theself the lock the sing in\n",
            "his visonger. The street, the blain in the arms overection of\n",
            "the it reeman the have the Hew the know this that coam out of A saith to the oar herself offer cust in the\n",
            "enot \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 42.72 min\n",
            "Iteration 14000 | Loss 2.06\n",
            "\n",
            "\n",
            "The Gaie sabme, \"into he so been hands was air.\n",
            "As hand from the shoe morroust resomened sat having to the mist\n",
            "in her prayen, by the poonsipity of too bernate someter when he heavy of poment conflines, \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 45.75 min\n",
            "Iteration 15000 | Loss 1.77\n",
            "\n",
            "\n",
            "The lang,\n",
            "for him. Eng Sour, and so the had been desicles resent, and ware\n",
            "her surson, with the face.\"\n",
            "\n",
            "\"I counted to, would they so Defarges!\" and out farge.\n",
            "\n",
            "\"You been a more discal Defere cart by the \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 48.78 min\n",
            "Iteration 16000 | Loss 1.94\n",
            "\n",
            "\n",
            "The\n",
            "datecompably only in me, and is it, and he assone to mock\n",
            "will be the way it this varaver, countity rageble be say put as leal as\n",
            "cutation, state this Prass't fortening Pour in the could said it weo \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 51.82 min\n",
            "Iteration 17000 | Loss 1.53\n",
            "\n",
            "\n",
            "The prissieall Jarless\n",
            "\n",
            "\n",
            "\"0ind of this has him mefepart. Nevement of him, I const is\n",
            "with air that soming the brame betweren at the were for a now steasting to be that three\n",
            "taken. I don't the beasing t \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 54.83 min\n",
            "Iteration 18000 | Loss 1.60\n",
            "\n",
            "\n",
            "The Doction, in the pret was bolly,\n",
            "amon the poor one and his she what out madame by every bachatie as\n",
            "he on sall now many in a foorwards at the parts bring and befloor\n",
            "conselfy on the recogse few three \n",
            "\n",
            "==================================================\n",
            "Time elapsed: 57.90 min\n",
            "Iteration 19000 | Loss 1.49\n",
            "\n",
            "\n",
            "The Defarge\n",
            "they had a he had a crorsal she gradgers of Parvee\n",
            "to be to the wrat the head might and herr to the leaved, he had been\n",
            "at have back on purpaught, made you have all his for busily\n",
            "full you s \n",
            "\n",
            "==================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT5sFp1owaj7"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}