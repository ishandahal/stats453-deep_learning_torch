{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rnn_LSTM_own_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfbbaYo0cZnRFcrP+tYwLv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishandahal/stats453-deep_learning_torch/blob/main/Rnn/Rnn_LSTM_own_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC7Sb326AkIh"
      },
      "source": [
        "### Using RNN with LSTM with own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YSpkRPQAylR"
      },
      "source": [
        "Using CSV text dataset for training a simple RNN for sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgP90PKYAr_n"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK0zgH08A-39"
      },
      "source": [
        "### General Settings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Rt5FKivBCrK"
      },
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIuDtlMwBF8y"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoFnhGRDBY-Q"
      },
      "source": [
        "Following cell with download csv-formatted file\n",
        "dataset [http://ai.stanford.edu/~amaas/data/sentiment/]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSsxnmoYBId7",
        "outputId": "23862c52-5cce-472f-a63d-2dcbdaf1e50d"
      },
      "source": [
        "!wget https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch08/movie_data.csv.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-28 16:23:40--  https://github.com/rasbt/python-machine-learning-book-2nd-edition/raw/master/code/ch08/movie_data.csv.gz\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch08/movie_data.csv.gz [following]\n",
            "--2020-12-28 16:23:41--  https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch08/movie_data.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26521894 (25M) [application/octet-stream]\n",
            "Saving to: ‘movie_data.csv.gz’\n",
            "\n",
            "movie_data.csv.gz   100%[===================>]  25.29M  82.5MB/s    in 0.3s    \n",
            "\n",
            "2020-12-28 16:23:41 (82.5 MB/s) - ‘movie_data.csv.gz’ saved [26521894/26521894]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNpZlR6-BPOz"
      },
      "source": [
        "!gunzip -f movie_data.csv.gz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1J9lb92B_o7"
      },
      "source": [
        "Checking the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PDViGkWACLfQ",
        "outputId": "a1b73364-1d0b-421a-aedc-20e2a27a581e"
      },
      "source": [
        "df = pd.read_csv('movie_data.csv')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NHC12QhCTsh"
      },
      "source": [
        "del df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmgDbRprCXr7"
      },
      "source": [
        "Define the Label and Text field formatters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oFHPkKvCtTJ"
      },
      "source": [
        "TEXT = data.Field(sequential=True,\n",
        "                  tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded _sequence\n",
        "\n",
        "LABEL = data.LabelField(dtype=torch.float)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwaQiDPmDEkk"
      },
      "source": [
        "Process the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqt6XCXJDGqV"
      },
      "source": [
        "fields = [('review', TEXT), ('sentiment', LABEL)]\n",
        "\n",
        "dataset = data.TabularDataset(\n",
        "    path='movie_data.csv', format='csv',\n",
        "    skip_header=True, fields=fields)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUXKRN7GDjFn"
      },
      "source": [
        "Split the dataset into training, validation and test partitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAhHW5L3Dnty",
        "outputId": "21bb54be-795e-49bc-b7c2-2c6422c02354"
      },
      "source": [
        "train_data, valid_data, test_data = dataset.split(\n",
        "    split_ratio=[0.75, 0.05, 0.2],\n",
        "    random_state=random.seed(RANDOM_SEED))\n",
        "\n",
        "print(f\"Num Train: {len(train_data)}\")\n",
        "print(f\"Num Valid: {len(valid_data)}\")\n",
        "print(f\"Num Test: {len(test_data)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Train: 37500\n",
            "Num Valid: 10000\n",
            "Num Test: 2500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTaFEEsnEGlE"
      },
      "source": [
        "Build the vocabulary based on the top \"VOCABULARY_SIZE\" words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6k49mouEESfN",
        "outputId": "860d3827-d2d8-4197-ef1e-23d6d4746cff"
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f\"Vocabulary size: {len(TEXT.vocab)}\")\n",
        "print(f\"Num of classes: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 20002\n",
            "Num of classes: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ULRi8ztEsF4",
        "outputId": "934b7593-a304-406a-e328-2668149154d5"
      },
      "source": [
        "LABEL.vocab.freqs"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'0': 18742, '1': 18758})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPNYHhxuEyCz"
      },
      "source": [
        "Make iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBtBRDXmE2Ny"
      },
      "source": [
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    sort_key=lambda x: len(x.review),\n",
        "    device=DEVICE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z3xj_MvGhcs",
        "outputId": "0cb9fd91-92bc-4784-f824-d05b95cdbbaf"
      },
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.review[0].size()}')\n",
        "    print(f'Target vector size: {batch.sentiment.size()}')\n",
        "    break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([512, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([52, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([74, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1VFLC88GnFX"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZSLs5XMGyVf"
      },
      "source": [
        "import torch.nn as nn \n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        # [sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to(dtype=torch.int64, device='cpu'))\n",
        "\n",
        "        # [sentence len, batch size, embedding size] => \n",
        "        # output: [sentence len, batch size, hidden dim]\n",
        "        # hidden: [1, batch size, hidden dim]\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "\n",
        "        return self.fc(hidden.squeeze(0)).view(-1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6M0_Mi0JS_i"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXfrWK8SJo7S"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcLppSoUJr8Z"
      },
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.review\n",
        "            logits = model(text, text_lengths)\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.sentiment.size(0)\n",
        "            correct_pred += (predicted_labels.long() == batch_data.sentiment.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LV4no_EJvwq",
        "outputId": "1098c529-8791-4eb8-b743-d05deaefa323"
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.review\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths)\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.sentiment)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 001/015 | Batch 000/293 | Cost: 0.6841\n",
            "Epoch: 001/015 | Batch 050/293 | Cost: 0.6912\n",
            "Epoch: 001/015 | Batch 100/293 | Cost: 0.6680\n",
            "Epoch: 001/015 | Batch 150/293 | Cost: 0.6818\n",
            "Epoch: 001/015 | Batch 200/293 | Cost: 0.6528\n",
            "Epoch: 001/015 | Batch 250/293 | Cost: 0.5883\n",
            "training accuracy: 68.47%\n",
            "valid accuracy: 68.07%\n",
            "Time elapsed: 0.33 min\n",
            "Epoch: 002/015 | Batch 000/293 | Cost: 0.6188\n",
            "Epoch: 002/015 | Batch 050/293 | Cost: 0.5914\n",
            "Epoch: 002/015 | Batch 100/293 | Cost: 0.5280\n",
            "Epoch: 002/015 | Batch 150/293 | Cost: 0.5492\n",
            "Epoch: 002/015 | Batch 200/293 | Cost: 0.5080\n",
            "Epoch: 002/015 | Batch 250/293 | Cost: 0.5789\n",
            "training accuracy: 77.17%\n",
            "valid accuracy: 76.97%\n",
            "Time elapsed: 0.65 min\n",
            "Epoch: 003/015 | Batch 000/293 | Cost: 0.4856\n",
            "Epoch: 003/015 | Batch 050/293 | Cost: 0.5013\n",
            "Epoch: 003/015 | Batch 100/293 | Cost: 0.5772\n",
            "Epoch: 003/015 | Batch 150/293 | Cost: 0.5387\n",
            "Epoch: 003/015 | Batch 200/293 | Cost: 0.4597\n",
            "Epoch: 003/015 | Batch 250/293 | Cost: 0.4706\n",
            "training accuracy: 79.60%\n",
            "valid accuracy: 79.52%\n",
            "Time elapsed: 0.98 min\n",
            "Epoch: 004/015 | Batch 000/293 | Cost: 0.5251\n",
            "Epoch: 004/015 | Batch 050/293 | Cost: 0.3910\n",
            "Epoch: 004/015 | Batch 100/293 | Cost: 0.4217\n",
            "Epoch: 004/015 | Batch 150/293 | Cost: 0.4291\n",
            "Epoch: 004/015 | Batch 200/293 | Cost: 0.4091\n",
            "Epoch: 004/015 | Batch 250/293 | Cost: 0.3612\n",
            "training accuracy: 82.58%\n",
            "valid accuracy: 81.41%\n",
            "Time elapsed: 1.32 min\n",
            "Epoch: 005/015 | Batch 000/293 | Cost: 0.4776\n",
            "Epoch: 005/015 | Batch 050/293 | Cost: 0.4252\n",
            "Epoch: 005/015 | Batch 100/293 | Cost: 0.5122\n",
            "Epoch: 005/015 | Batch 150/293 | Cost: 0.3316\n",
            "Epoch: 005/015 | Batch 200/293 | Cost: 0.3351\n",
            "Epoch: 005/015 | Batch 250/293 | Cost: 0.4041\n",
            "training accuracy: 81.73%\n",
            "valid accuracy: 80.49%\n",
            "Time elapsed: 1.65 min\n",
            "Epoch: 006/015 | Batch 000/293 | Cost: 0.4077\n",
            "Epoch: 006/015 | Batch 050/293 | Cost: 0.3912\n",
            "Epoch: 006/015 | Batch 100/293 | Cost: 0.5628\n",
            "Epoch: 006/015 | Batch 150/293 | Cost: 0.3927\n",
            "Epoch: 006/015 | Batch 200/293 | Cost: 0.3868\n",
            "Epoch: 006/015 | Batch 250/293 | Cost: 0.3728\n",
            "training accuracy: 85.73%\n",
            "valid accuracy: 83.74%\n",
            "Time elapsed: 1.99 min\n",
            "Epoch: 007/015 | Batch 000/293 | Cost: 0.2942\n",
            "Epoch: 007/015 | Batch 050/293 | Cost: 0.3209\n",
            "Epoch: 007/015 | Batch 100/293 | Cost: 0.4031\n",
            "Epoch: 007/015 | Batch 150/293 | Cost: 0.2875\n",
            "Epoch: 007/015 | Batch 200/293 | Cost: 0.4341\n",
            "Epoch: 007/015 | Batch 250/293 | Cost: 0.2766\n",
            "training accuracy: 86.65%\n",
            "valid accuracy: 85.08%\n",
            "Time elapsed: 2.33 min\n",
            "Epoch: 008/015 | Batch 000/293 | Cost: 0.2683\n",
            "Epoch: 008/015 | Batch 050/293 | Cost: 0.3026\n",
            "Epoch: 008/015 | Batch 100/293 | Cost: 0.3588\n",
            "Epoch: 008/015 | Batch 150/293 | Cost: 0.2809\n",
            "Epoch: 008/015 | Batch 200/293 | Cost: 0.3742\n",
            "Epoch: 008/015 | Batch 250/293 | Cost: 0.3656\n",
            "training accuracy: 87.58%\n",
            "valid accuracy: 85.70%\n",
            "Time elapsed: 2.67 min\n",
            "Epoch: 009/015 | Batch 000/293 | Cost: 0.3125\n",
            "Epoch: 009/015 | Batch 050/293 | Cost: 0.3846\n",
            "Epoch: 009/015 | Batch 100/293 | Cost: 0.3397\n",
            "Epoch: 009/015 | Batch 150/293 | Cost: 0.2499\n",
            "Epoch: 009/015 | Batch 200/293 | Cost: 0.2786\n",
            "Epoch: 009/015 | Batch 250/293 | Cost: 0.3497\n",
            "training accuracy: 88.74%\n",
            "valid accuracy: 85.94%\n",
            "Time elapsed: 3.01 min\n",
            "Epoch: 010/015 | Batch 000/293 | Cost: 0.1557\n",
            "Epoch: 010/015 | Batch 050/293 | Cost: 0.4214\n",
            "Epoch: 010/015 | Batch 100/293 | Cost: 0.1993\n",
            "Epoch: 010/015 | Batch 150/293 | Cost: 0.3025\n",
            "Epoch: 010/015 | Batch 200/293 | Cost: 0.2239\n",
            "Epoch: 010/015 | Batch 250/293 | Cost: 0.3423\n",
            "training accuracy: 88.92%\n",
            "valid accuracy: 85.68%\n",
            "Time elapsed: 3.35 min\n",
            "Epoch: 011/015 | Batch 000/293 | Cost: 0.2061\n",
            "Epoch: 011/015 | Batch 050/293 | Cost: 0.2789\n",
            "Epoch: 011/015 | Batch 100/293 | Cost: 0.2200\n",
            "Epoch: 011/015 | Batch 150/293 | Cost: 0.4777\n",
            "Epoch: 011/015 | Batch 200/293 | Cost: 0.3674\n",
            "Epoch: 011/015 | Batch 250/293 | Cost: 0.2829\n",
            "training accuracy: 76.69%\n",
            "valid accuracy: 75.03%\n",
            "Time elapsed: 3.69 min\n",
            "Epoch: 012/015 | Batch 000/293 | Cost: 0.5037\n",
            "Epoch: 012/015 | Batch 050/293 | Cost: 0.4181\n",
            "Epoch: 012/015 | Batch 100/293 | Cost: 0.3199\n",
            "Epoch: 012/015 | Batch 150/293 | Cost: 0.2285\n",
            "Epoch: 012/015 | Batch 200/293 | Cost: 0.2327\n",
            "Epoch: 012/015 | Batch 250/293 | Cost: 0.3309\n",
            "training accuracy: 90.33%\n",
            "valid accuracy: 86.49%\n",
            "Time elapsed: 4.04 min\n",
            "Epoch: 013/015 | Batch 000/293 | Cost: 0.1973\n",
            "Epoch: 013/015 | Batch 050/293 | Cost: 0.2727\n",
            "Epoch: 013/015 | Batch 100/293 | Cost: 0.2224\n",
            "Epoch: 013/015 | Batch 150/293 | Cost: 0.1948\n",
            "Epoch: 013/015 | Batch 200/293 | Cost: 0.2470\n",
            "Epoch: 013/015 | Batch 250/293 | Cost: 0.2979\n",
            "training accuracy: 89.71%\n",
            "valid accuracy: 85.95%\n",
            "Time elapsed: 4.38 min\n",
            "Epoch: 014/015 | Batch 000/293 | Cost: 0.2193\n",
            "Epoch: 014/015 | Batch 050/293 | Cost: 0.2329\n",
            "Epoch: 014/015 | Batch 100/293 | Cost: 0.1606\n",
            "Epoch: 014/015 | Batch 150/293 | Cost: 0.1898\n",
            "Epoch: 014/015 | Batch 200/293 | Cost: 0.3458\n",
            "Epoch: 014/015 | Batch 250/293 | Cost: 0.2265\n",
            "training accuracy: 90.78%\n",
            "valid accuracy: 86.76%\n",
            "Time elapsed: 4.73 min\n",
            "Epoch: 015/015 | Batch 000/293 | Cost: 0.2217\n",
            "Epoch: 015/015 | Batch 050/293 | Cost: 0.2522\n",
            "Epoch: 015/015 | Batch 100/293 | Cost: 0.2159\n",
            "Epoch: 015/015 | Batch 150/293 | Cost: 0.2417\n",
            "Epoch: 015/015 | Batch 200/293 | Cost: 0.2524\n",
            "Epoch: 015/015 | Batch 250/293 | Cost: 0.1601\n",
            "training accuracy: 91.43%\n",
            "valid accuracy: 87.29%\n",
            "Time elapsed: 5.07 min\n",
            "Total Training Time: 5.07 min\n",
            "Test accuracy: 88.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoakcM3IJ11k"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    # based on:\n",
        "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
        "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZqoerSiLv3C",
        "outputId": "42b2cef0-276d-4bd6-b06f-5630bc5047cf"
      },
      "source": [
        "print('Probability positive:')\n",
        "1-predict_sentiment(model, \"This is such an awesome movie, I really love it!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability positive:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7822760194540024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upGhrBs-Lz8Q",
        "outputId": "0d006e54-8b45-41b9-aa18-73f47d6de0e9"
      },
      "source": [
        "print('Probability negative:')\n",
        "predict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability negative:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7763678431510925"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoLIjiKRL26E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}