{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Embedding, Linear, LSTM, Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CharacterDataset(Dataset):\n",
    "    \"\"\"Custom dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text that will be used to create the entire dataset.\n",
    "    \n",
    "    window_size : int\n",
    "        Number of characters to use as input features.\n",
    "    \n",
    "    vocab_size : int\n",
    "        Number of characters in the vocabulary. Note that the last character\n",
    "        is always reserved for a special \"~\" out-of-vocabulary character.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ch2ix : defaultdict\n",
    "        Mapping the character to the position of that character in the\n",
    "        vocabulary. Note that all characters that are not in the vocabulary\n",
    "        will be mapped into the index `vocab_size - 1`.\n",
    "\n",
    "    ix2ch : dict\n",
    "        Mapping from the character position in the vocabulary to the actual\n",
    "        character.\n",
    "\n",
    "    vocabulary : list \n",
    "        List of all characters. `len(vocabulary) == vocab_size`.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, window_size=1, vocab_size=50):\n",
    "        self.text = text.replace(\"\\n\", \" \")\n",
    "        self.window_size = window_size\n",
    "        self.ch2ix = defaultdict(lambda: vocab_size - 1)\n",
    "\n",
    "        most_common_ch2ix = {\n",
    "            x[0]: i\n",
    "            for i, x in enumerate(Counter(self.text).most_common()[: (vocab_size - 1)])\n",
    "        }\n",
    "\n",
    "        self.ch2ix.update(most_common_ch2ix)\n",
    "        self.ch2ix[\"~\"] = vocab_size - 1\n",
    "\n",
    "        self.ix2ch = {v: k for k, v in self.ch2ix.items()}\n",
    "        self.vocabulary = [self.ix2ch[i] for i in range(vocab_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.window_size\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        X = torch.LongTensor(\n",
    "            [self.ch2ix[c] for c in self.text[ix : ix + self.window_size]]\n",
    "        )\n",
    "        y = self.ch2ix[self.text[ix + self.window_size]]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class Network(Module):\n",
    "    \"\"\"Custom network predicting the next character of a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        The number of characters in the vocabulary.\n",
    "    \n",
    "    embedding_dim : int\n",
    "        Dimension of the character embedding vectors.\n",
    "    \n",
    "    dense_dim : int\n",
    "        Number of neurons in the linear layer that follows the LSTM.\n",
    "\n",
    "    hidden_dim : int\n",
    "        Size of the LSTM hidden state.\n",
    "\n",
    "    max_norm : int \n",
    "        If any of the embedding vectors has a higher L2 norm than `max_norm`\n",
    "        it is rescaled.\n",
    "\n",
    "    n_layers : int\n",
    "        Number of the layers of the LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim=2,\n",
    "        dense_dim=32,\n",
    "        hidden_dim=8,\n",
    "        max_norm=2,\n",
    "        n_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "                    vocab_size,\n",
    "                    embedding_dim,\n",
    "                    padding_idx=vocab_size - 1,\n",
    "                    norm_type=2,\n",
    "                    max_norm=max_norm,\n",
    "        )\n",
    "        self.lstm = LSTM(\n",
    "            embedding_dim, hidden_dim, batch_first=True, num_layers=n_layers\n",
    "        )\n",
    "        self.linear_1 = Linear(hidden_dim, dense_dim)\n",
    "        self.linear_2 = Linear(dense_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape `(n_samples, wndow_size)` of dtype\n",
    "            `torch.int64`.\n",
    "\n",
    "        h, c : torch.Tensor or None\n",
    "            Hidden states of the LSTM.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Tensor of shape `(n_samples, vocab_size)`.\n",
    "        \n",
    "        h, c : torch.Tensor or None\n",
    "            Hidden states of the LSTM.\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x) # (n_samples, window_size, embedding_dim)\n",
    "        if h is not None and c is not None:\n",
    "            _, (h, c) = self.lstm(emb, (h, c))\n",
    "        else:\n",
    "            _, (h, c) = self.lstm(emb) # (n_layers, n_samples, hidden_dim)\n",
    "\n",
    "        h_mean = h.mean(dim=0) # (n_samples, hidden_dim)\n",
    "        x = self.linear_1(h_mean) # (n_samples, dense_dim)\n",
    "        logits = self.linear_2(x) # (n_samples, vocab_size)\n",
    "\n",
    "        return logits, h, c\n",
    "\n",
    "def compute_loss(cal, net, dataloader):\n",
    "    \"\"\"Compute average loss over the dataset.\"\"\"\n",
    "    net.eval()\n",
    "    all_losses = []\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        logits, _, _ = net(X_batch)\n",
    "\n",
    "        all_losses.append(cal(logits, y_batch).item())\n",
    "\n",
    "    return np.mean(all_losses)\n",
    "\n",
    "def generate_text(n_chars, net, dataset, initial_text='Hello', random_state=None):\n",
    "    \"\"\"Generate text with the character-level model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        Number of characters to generate.\n",
    "\n",
    "    net : Module\n",
    "        Character-level model.\n",
    "\n",
    "    dataset : CharacterDataset\n",
    "        Instance of the `CharacterDataset`.\n",
    "\n",
    "    initial_text : str\n",
    "        The starting text to be used as the initial condition for the model.\n",
    "\n",
    "    random_state : None or int\n",
    "        If not None, then the result is reproducible.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : str\n",
    "        Generated text.\n",
    "    \"\"\"\n",
    "    if not initial_text:\n",
    "        raise ValueError(\"You need to specify the initial text\")\n",
    "\n",
    "    res = initial_text\n",
    "    net.eval()\n",
    "    h, c = None, None\n",
    "\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    for _ in range(n_chars):\n",
    "        previous_chars = initial_text if res == initial_text else res[-1]\n",
    "        features = torch.LongTensor([[dataset.ch2ix[c] for c in previous_chars]])\n",
    "        logits, h, c = net(features, h, c)\n",
    "        probas = F.softmax(logits[0], dim=0).detach().numpy()\n",
    "        new_ch = np.random.choice(dataset.vocabulary, p=probas)\n",
    "        res += new_ch\n",
    "\n",
    "    return res\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"text.txt\", \"r\") as f:\n",
    "        text = \"\\n\".join(f.readlines())\n",
    "\n",
    "    # Hyperparameters model\n",
    "    vocab_size = 70\n",
    "    window_size = 10\n",
    "    embedding_dim = 2\n",
    "    hidden_dim = 32\n",
    "    dense_dim = 32\n",
    "    n_layers = 1\n",
    "    max_norm = 2\n",
    "\n",
    "    # Training config \n",
    "    n_epochs = 25\n",
    "    train_val_split = 0.8\n",
    "    batch_size = 128\n",
    "    random_state = 13\n",
    "\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    dataset = CharacterDataset(text, window_size=window_size, vocab_size=vocab_size)\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "    split_ix = int(n_samples * train_val_split)\n",
    "\n",
    "    train_indices, valid_indices = np.arange(split_ix), np.arange(split_ix, n_samples)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, sampler=SubsetRandomSampler(train_indices), batch_size=batch_size\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        dataset, sampler=SubsetRandomSampler(valid_indices), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    net = Network(\n",
    "        vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        dense_dim=dense_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        max_norm=max_norm, \n",
    "        n_layers=n_layers\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr=1e-2,\n",
    "    )\n",
    "\n",
    "    emb_history = []\n",
    "\n",
    "    for e in range(n_epochs + 1):\n",
    "        net.train()\n",
    "        for X_batch, y_batch in tqdm(train_dataloader):\n",
    "            if e == 0:\n",
    "                break\n",
    "\n",
    "            probas, _, _ = net(X_batch)\n",
    "            loss = loss_f(probas, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = compute_loss(loss_f, net, train_dataloader)\n",
    "        valid_loss = compute_loss(loss_f, net, valid_dataloader)\n",
    "        print(f\"Epoch: {e}, Train loss: {train_loss=:.3f}, Valid loss: {val_loss=:.3f}\")\n",
    "\n",
    "        # Generate one sentence\n",
    "        initial_text = \"Let's give it a go \"\n",
    "        generated_text = generate_text(100, net, dataset, random_state=random_state,\\\n",
    "                                        initial_text=initial_text)\n",
    "        print(generate_text)\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        weights = net.embedding.weight.detach().clone().numpy()\n",
    "\n",
    "        df = pd.DataFrame(weights, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
    "        df[\"epoch\"] = e\n",
    "        df[\"character\"] = dataset.vocabulary\n",
    "\n",
    "        emb_history.append(df)\n",
    "\n",
    "final_df = pd.concat(emb_history)\n",
    "# final_df.csv(\"res.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Embedding, Linear, LSTM, Module\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CharacterDataset(Dataset):\n",
    "    \"\"\"Custom dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Input text that will be used to create the entire database.\n",
    "    window_size : int\n",
    "        Number of characters to use as input features.\n",
    "    vocab_size : int\n",
    "        Number of characters in the vocabulary. Note that the last character\n",
    "        is always reserved for a special \"~\" out-of-vocabulary character.\n",
    "    Attributes\n",
    "    ----------\n",
    "    ch2ix : defaultdict\n",
    "        Mapping from the character to the position of that character in the\n",
    "        vocabulary. Note that all characters that are not in the vocabulary\n",
    "        will get mapped into the index `vocab_size - 1`.\n",
    "    ix2ch : dict\n",
    "        Mapping from the character position in the vocabulary to the actual\n",
    "        character.\n",
    "    vocabulary : list\n",
    "        List of all characters. `len(vocabulary) == vocab_size`.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, window_size=1, vocab_size=50):\n",
    "        self.text = text.replace(\"\\n\", \" \")\n",
    "        self.window_size = window_size\n",
    "        self.ch2ix = defaultdict(lambda: vocab_size - 1)\n",
    "\n",
    "        most_common_ch2ix = {\n",
    "            x[0]: i\n",
    "            for i, x in enumerate(Counter(self.text).most_common()[: (vocab_size - 1)])\n",
    "        }\n",
    "        self.ch2ix.update(most_common_ch2ix)\n",
    "        self.ch2ix[\"~\"] = vocab_size - 1\n",
    "\n",
    "        self.ix2ch = {v: k for k, v in self.ch2ix.items()}\n",
    "        self.vocabulary = [self.ix2ch[i] for i in range(vocab_size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.window_size\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        X = torch.LongTensor(\n",
    "            [self.ch2ix[c] for c in self.text[ix : ix + self.window_size]]\n",
    "        )\n",
    "        y = self.ch2ix[self.text[ix + self.window_size]]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "class Network(Module):\n",
    "    \"\"\"Custom network predicting the next character of a string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        The number of charactesr in the vocabulary.\n",
    "    embedding_dim : int\n",
    "        Dimension of the character embedding vectors.\n",
    "    dense_dim : int\n",
    "        Number of neurons in the linear layer that follows the LSTM.\n",
    "    hidden_dim : int\n",
    "        Size of the LSTM hidden state.\n",
    "    max_norm : int\n",
    "        If any of the embedding vectors has a higher L2 norm than `max_norm`\n",
    "        it is rescaled.\n",
    "    n_layers : int\n",
    "        Number of the layers of the LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim=2,\n",
    "        dense_dim=32,\n",
    "        hidden_dim=8,\n",
    "        max_norm=2,\n",
    "        n_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "                vocab_size,\n",
    "                embedding_dim,\n",
    "                padding_idx=vocab_size - 1,\n",
    "                norm_type=2,\n",
    "                max_norm=max_norm,\n",
    "        )\n",
    "        self.lstm = LSTM(\n",
    "                embedding_dim, hidden_dim, batch_first=True, num_layers=n_layers\n",
    "        )\n",
    "        self.linear_1 = Linear(hidden_dim, dense_dim)\n",
    "        self.linear_2 = Linear(dense_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        \"\"\"Run the forward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape `(n_samples, window_size)` of dtype\n",
    "            `torch.int64`.\n",
    "        h, c : torch.Tensor or None\n",
    "            Hidden states of the LSTM.\n",
    "        Returns\n",
    "        -------\n",
    "        logits : torch.Tensor\n",
    "            Tensor of shape `(n_samples, vocab_size)`.\n",
    "        h, c : torch.Tensor or None\n",
    "            Hidden states of the LSTM.\n",
    "        \"\"\"\n",
    "        emb = self.embedding(x)  # (n_samples, window_size, embedding_dim)\n",
    "        if h is not None and c is not None:\n",
    "            _, (h, c) = self.lstm(emb, (h, c))\n",
    "        else:\n",
    "            _, (h, c) = self.lstm(emb)  # (n_layers, n_samples, hidden_dim)\n",
    "\n",
    "        h_mean = h.mean(dim=0)  # (n_samples, hidden_dim)\n",
    "        x = self.linear_1(h_mean)  # (n_samples, dense_dim)\n",
    "        logits = self.linear_2(x)  # (n_samples, vocab_size)\n",
    "\n",
    "        return logits, h, c\n",
    "\n",
    "def compute_loss(cal, net, dataloader):\n",
    "    \"\"\"Computer average loss over a dataset.\"\"\"\n",
    "    net.eval()\n",
    "    all_losses = []\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        probs, _, _ = net(X_batch)\n",
    "\n",
    "        all_losses.append(cal(probs, y_batch).item())\n",
    "\n",
    "    return np.mean(all_losses)\n",
    "\n",
    "def generate_text(n_chars, net, dataset, initial_text=\"Hello\", random_state=None):\n",
    "    \"\"\"Generate text with the character-level model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_chars : int\n",
    "        Number of characters to generate.\n",
    "    net : Module\n",
    "        Character-level model.\n",
    "    dataset : CharacterDataset\n",
    "        Instance of the `CharacterDataset`.\n",
    "    initial_text : str\n",
    "        The starting text to be used as the initial condition for the model.\n",
    "    random_state : None or int\n",
    "        If not None, then the result is reproducible.\n",
    "    Returns\n",
    "    -------\n",
    "    res : str\n",
    "        Generated text.\n",
    "    \"\"\"\n",
    "    if not initial_text:\n",
    "        raise ValueError(\"You need to specify the initial text\")\n",
    "\n",
    "    res = initial_text\n",
    "    net.eval()\n",
    "    h, c = None, None\n",
    "\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    for _ in range(n_chars):\n",
    "        previous_chars = initial_text if res == initial_text else res[-1]\n",
    "        features = torch.LongTensor([[dataset.ch2ix[c] for c in previous_chars]])\n",
    "        logits, h, c = net(features, h, c)\n",
    "        probs = F.softmax(logits[0], dim=0).detach().numpy()\n",
    "        new_ch = np.random.choice(dataset.vocabulary, p=probs)\n",
    "        res += new_ch\n",
    "\n",
    "    return res\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"text.txt\", \"r\") as f:\n",
    "        text = \"\\n\".join(f.readlines())\n",
    "\n",
    "    # Hyperparameters model\n",
    "    vocab_size = 70\n",
    "    window_size = 10\n",
    "    embedding_dim = 2\n",
    "    hidden_dim = 16\n",
    "    dense_dim = 32\n",
    "    n_layers = 1\n",
    "    max_norm = 2\n",
    "\n",
    "    # Training config\n",
    "    n_epochs = 25\n",
    "    train_val_split = 0.8\n",
    "    batch_size = 128\n",
    "    random_state = 13\n",
    "\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    dataset = CharacterDataset(text, window_size=window_size, vocab_size=vocab_size)\n",
    "\n",
    "    n_samples = len(dataset)\n",
    "    split_ix = int(n_samples * train_val_split)\n",
    "\n",
    "    train_indices, val_indices = np.arange(split_ix), np.arange(split_ix, n_samples)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            dataset, sampler=SubsetRandomSampler(train_indices), batch_size=batch_size\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "            dataset, sampler=SubsetRandomSampler(val_indices), batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    net = Network(\n",
    "            vocab_size,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_layers=n_layers,\n",
    "            dense_dim=dense_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            max_norm=max_norm,\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(\n",
    "            net.parameters(),\n",
    "            lr=1e-2,\n",
    "    )\n",
    "\n",
    "    emb_history = []\n",
    "\n",
    "    for e in range(n_epochs + 1):\n",
    "        net.train()\n",
    "        for X_batch, y_batch in tqdm(train_dataloader):\n",
    "            if e == 0:\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs, _, _ = net(X_batch)\n",
    "            loss = loss_f(probs, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = compute_loss(loss_f, net, train_dataloader)\n",
    "        val_loss = compute_loss(loss_f, net, val_dataloader)\n",
    "        print(f\"Epoch: {e}, {train_loss=:.3f}, {val_loss=:.3f}\")\n",
    "\n",
    "        # Generate one sentence\n",
    "        initial_text = \"I hope it works \"\n",
    "        generated_text = generate_text(\n",
    "            100, net, dataset, initial_text=initial_text, random_state=random_state\n",
    "        )\n",
    "        print(generated_text)\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        weights = net.embedding.weight.detach().clone().numpy()\n",
    "\n",
    "        df = pd.DataFrame(weights, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\n",
    "        df[\"epoch\"] = e\n",
    "        df[\"character\"] = dataset.vocabulary\n",
    "\n",
    "        emb_history.append(df)\n",
    "\n",
    "final_df = pd.concat(emb_history)\n",
    "# final_df.to_csv(\"res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "silver-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "liable-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"res.csv\")\n",
    "last_epoch = df[\"epoch\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preceding-tomato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee190a07f264ba8bdd1bf6f3467a75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='epoch', max=25), Output()), _do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@ipywidgets.interact\n",
    "def f(epoch=ipywidgets.IntSlider(min=0, max=last_epoch , continuous_update=False)):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.set_xlim([-2, 2])\n",
    "    ax.set_ylim([-2, 2])\n",
    "    df_iter = df[df[\"epoch\"] == epoch]\n",
    "    df_iter.plot(kind='scatter', x='dim_0',y='dim_1', ax=ax, c=\"red\")\n",
    "    df_iter[['dim_0','dim_1','character']].apply(lambda row:\n",
    "                                                 ax.text(row[\"dim_0\"] + 0.02,\n",
    "                                                         row[\"dim_1\"] + 0.01,\n",
    "                                                         row[\"character\"],\n",
    "                                                         fontsize=18),\n",
    "                                                 axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tissue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
